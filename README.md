ğŸ“˜ Text Classification with BERT on AG News

This repository contains code, model, and resources for **fine-tuning BERT on the AG News dataset** using [Hugging Face Transformers](https://huggingface.co/transformers/).
The goal is to classify news articles into one of **4 categories**:

1. World ğŸŒ
2. Sports ğŸ…
3. Business ğŸ’¼
4. Science/Technology ğŸ”¬

ğŸš€ Project Overview

We fine-tuned BERT (bert-base-uncased) on the AG News dataset for a text classification task.
This project demonstrates the end-to-end workflow of NLP model fine-tuning:

1. Load Datase â†’ AG News dataset (train + test splits).
2. Preprocess Data â†’ Tokenization using BERT tokenizer.
3. Prepare Dataloaders â†’ Batch data with padding/collation.
4.Fine-Tune BERT â†’ Train on a subset for efficiency.
5. Evaluate Model â†’ Compute accuracy on test set.
6. Save Model â†’ Export weights + tokenizer for later inference.

ğŸ“¦ Dependencies

Main Python libraries used:

[transformers](https://pypi.org/project/transformers/)
[datasets](https://pypi.org/project/datasets/)
[evaluate](https://pypi.org/project/evaluate/)
[torch](https://pytorch.org/)
[tqdm](https://pypi.org/project/tqdm/)

You can install them directly with:

```bash
pip install torch transformers datasets evaluate tqdm
```

ğŸ‹ï¸ Training Procedure

We fine-tuned BERT with the following setup:

* Batch size: 16
* Max sequence length: 128 tokens
* Optimizer: AdamW
* Learning rate scheduler: Linear
* Epochs: 1 (light run for demo; can increase)
* Device: GPU (Tesla T4 in Google Colab)

Training code snippet:

```python
for batch in train_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    batch["labels"] = batch.pop("label")

    outputs = model(**batch)
    loss = outputs.loss

    loss.backward()
    optimizer.step()
    lr_scheduler.step()
    optimizer.zero_grad()


ğŸ“Š Results

After fine-tuning on a small subset of AG News, we achieved:

* Accuracy: \~92% âœ…

This demonstrates that BERT generalizes well even with limited training.


ğŸ’¾ Saving & Loading the Model

We saved the trained model and tokenizer:

```python
model.save_pretrained("./bert_agnews_model")
tokenizer.save_pretrained("./bert_agnews_model")
```

To load it later:

```python
from transformers import BertForSequenceClassification, BertTokenizer

model = BertForSequenceClassification.from_pretrained("./bert_agnews_model")
tokenizer = BertTokenizer.from_pretrained("./bert_agnews_model")
```

---

ğŸ” Inference

Example of running predictions on your own sentence:

```python
text = "NASA launches a new satellite to study climate change."
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
outputs = model(**inputs)
predicted_class = outputs.logits.argmax(dim=-1).item()

labels = ["World", "Sports", "Business", "Sci/Tech"]
print("Predicted label:", labels[predicted_class])
```

Output:

```
Predicted label: Sci/Tech
```


ğŸ“Œ Next Steps

* Train for more epochs on the **full dataset**.
* Try different models (`distilbert-base-uncased`, `roberta-base`, etc.).
* Push the model to [ğŸ¤— Hugging Face Hub](https://huggingface.co/) for easy sharing.


